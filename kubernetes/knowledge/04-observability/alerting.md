# Kubernetes æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ

## æ¦‚è¿°

æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿæ˜¯ Kubernetes å¯è§‚æµ‹æ€§çš„æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡æ™ºèƒ½åŒ–çš„è§„åˆ™å¼•æ“ã€é™å™ªç®—æ³•å’Œå¤šæ¸ é“é€šçŸ¥æœºåˆ¶ï¼Œä¸ºè¿ç»´å›¢é˜Ÿæä¾›åŠæ—¶ã€å‡†ç¡®ã€å¯æ“ä½œçš„å‘Šè­¦ä¿¡æ¯ã€‚

### å‘Šè­¦ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    subgraph "æ•°æ®æº"
        PROMETHEUS[Prometheus æŒ‡æ ‡]
        LOGS[æ—¥å¿—æ•°æ®]
        TRACES[é“¾è·¯æ•°æ®]
        EVENTS[K8s äº‹ä»¶]
        EXTERNAL[å¤–éƒ¨ç³»ç»Ÿ]
    end
    
    subgraph "å‘Šè­¦å¼•æ“"
        PROMETHEUS_ALERTING[Prometheus å‘Šè­¦è§„åˆ™]
        ELASTALERT[ElastAlert]
        CUSTOM_RULES[è‡ªå®šä¹‰è§„åˆ™å¼•æ“]
        ML_DETECTION[æœºå™¨å­¦ä¹ æ£€æµ‹]
        ANOMALY_DETECTION[å¼‚å¸¸æ£€æµ‹]
    end
    
    subgraph "å‘Šè­¦å¤„ç†"
        ALERTMANAGER[Alertmanager]
        GROUPING[å‘Šè­¦åˆ†ç»„]
        SILENCING[é™é»˜è§„åˆ™]
        INHIBITION[æŠ‘åˆ¶è§„åˆ™]
        ROUTING[è·¯ç”±è§„åˆ™]
    end
    
    subgraph "æ™ºèƒ½åŒ–å¤„ç†"
        DEDUPLICATION[å»é‡å¤„ç†]
        CORRELATION[å…³è”åˆ†æ]
        ESCALATION[å‡çº§ç­–ç•¥]
        AUTO_RESOLUTION[è‡ªåŠ¨è§£å†³]
        CONTEXT_ENRICHMENT[ä¸Šä¸‹æ–‡å¢å¼º]
    end
    
    subgraph "é€šçŸ¥æ¸ é“"
        EMAIL[é‚®ä»¶é€šçŸ¥]
        SLACK[Slack]
        TEAMS[Microsoft Teams]
        WEBHOOK[Webhook]
        SMS[çŸ­ä¿¡é€šçŸ¥]
        PHONE[ç”µè¯é€šçŸ¥]
        PAGERDUTY[PagerDuty]
        INCIDENT_TOOLS[äº‹ä»¶ç®¡ç†å·¥å…·]
    end
    
    PROMETHEUS --> PROMETHEUS_ALERTING
    LOGS --> ELASTALERT
    TRACES --> CUSTOM_RULES
    EVENTS --> ML_DETECTION
    EXTERNAL --> ANOMALY_DETECTION
    
    PROMETHEUS_ALERTING --> ALERTMANAGER
    ELASTALERT --> ALERTMANAGER
    CUSTOM_RULES --> ALERTMANAGER
    ML_DETECTION --> ALERTMANAGER
    ANOMALY_DETECTION --> ALERTMANAGER
    
    ALERTMANAGER --> GROUPING
    GROUPING --> SILENCING
    SILENCING --> INHIBITION
    INHIBITION --> ROUTING
    
    ROUTING --> DEDUPLICATION
    DEDUPLICATION --> CORRELATION
    CORRELATION --> ESCALATION
    ESCALATION --> AUTO_RESOLUTION
    AUTO_RESOLUTION --> CONTEXT_ENRICHMENT
    
    CONTEXT_ENRICHMENT --> EMAIL
    CONTEXT_ENRICHMENT --> SLACK
    CONTEXT_ENRICHMENT --> TEAMS
    CONTEXT_ENRICHMENT --> WEBHOOK
    CONTEXT_ENRICHMENT --> SMS
    CONTEXT_ENRICHMENT --> PHONE
    CONTEXT_ENRICHMENT --> PAGERDUTY
    CONTEXT_ENRICHMENT --> INCIDENT_TOOLS
```

## Prometheus å‘Šè­¦è§„åˆ™

### åŸºç¡€è®¾æ–½å‘Šè­¦

```yaml
# infrastructure-alerts.yaml
groups:
  - name: infrastructure.rules
    interval: 30s
    rules:
      # èŠ‚ç‚¹çŠ¶æ€å‘Šè­¦
      - alert: NodeDown
        expr: up{job="node-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
          component: node
        annotations:
          summary: "Node {{ $labels.instance }} is down"
          description: |
            Node {{ $labels.instance }} has been down for more than 1 minute.
            
            Current status: DOWN
            Last seen: {{ $value }}
            
            Immediate actions:
            1. Check node physical/virtual machine status
            2. Verify network connectivity
            3. Check system logs for errors
            4. Consider node replacement if hardware failure
          runbook_url: "https://runbooks.company.com/infrastructure/node-down"
          dashboard_url: "https://grafana.company.com/d/node-overview"
      
      # CPU ä½¿ç”¨ç‡å‘Šè­¦
      - alert: HighCPUUsage
        expr: |
          (1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: infrastructure
          component: cpu
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: |
            CPU usage on node {{ $labels.instance }} is {{ $value | humanizePercentage }}.
            
            Current CPU usage: {{ $value | humanizePercentage }}
            Threshold: 80%
            Duration: > 5 minutes
            
            Investigation steps:
            1. Check top processes consuming CPU
            2. Review CPU usage trends
            3. Consider scaling or optimization
          query: '(1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100'
      
      # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
      - alert: HighMemoryUsage
        expr: |
          (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 85
        for: 3m
        labels:
          severity: warning
          team: infrastructure
          component: memory
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: |
            Memory usage on node {{ $labels.instance }} is {{ $value | humanizePercentage }}.
            
            Available memory: {{ with query "node_memory_MemAvailable_bytes{instance='$labels.instance'}" }}{{ . | first | value | humanizeBytes }}{{ end }}
            Total memory: {{ with query "node_memory_MemTotal_bytes{instance='$labels.instance'}" }}{{ . | first | value | humanizeBytes }}{{ end }}
            
            Recommended actions:
            1. Identify memory-intensive processes
            2. Check for memory leaks
            3. Consider adding more memory or optimization
      
      # ç£ç›˜ç©ºé—´å‘Šè­¦
      - alert: HighDiskUsage
        expr: |
          (1 - node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes) * 100 > 85
        for: 1m
        labels:
          severity: warning
          team: infrastructure
          component: disk
        annotations:
          summary: "High disk usage on {{ $labels.instance }}:{{ $labels.mountpoint }}"
          description: |
            Disk usage on {{ $labels.instance }}:{{ $labels.mountpoint }} is {{ $value | humanizePercentage }}.
            
            Available space: {{ with query "node_filesystem_avail_bytes{instance='$labels.instance',mountpoint='$labels.mountpoint'}" }}{{ . | first | value | humanizeBytes }}{{ end }}
            Total space: {{ with query "node_filesystem_size_bytes{instance='$labels.instance',mountpoint='$labels.mountpoint'}" }}{{ . | first | value | humanizeBytes }}{{ end }}
            
            Cleanup recommendations:
            1. Remove old log files
            2. Clean temporary files
            3. Archive or compress large files
            4. Consider storage expansion
      
      # ç£ç›˜ I/O å‘Šè­¦
      - alert: HighDiskIOWait
        expr: rate(node_cpu_seconds_total{mode="iowait"}[5m]) * 100 > 20
        for: 5m
        labels:
          severity: warning
          team: infrastructure
          component: disk
        annotations:
          summary: "High disk I/O wait on {{ $labels.instance }}"
          description: |
            I/O wait time on {{ $labels.instance }} is {{ $value | humanizePercentage }}.
            This indicates disk performance issues.
            
            Investigation steps:
            1. Check disk utilization and queue depth
            2. Identify I/O intensive processes
            3. Consider storage optimization or upgrade
      
      # ç½‘ç»œé”™è¯¯å‘Šè­¦
      - alert: HighNetworkErrors
        expr: |
          rate(node_network_receive_errs_total[5m]) + rate(node_network_transmit_errs_total[5m]) > 10
        for: 2m
        labels:
          severity: warning
          team: infrastructure
          component: network
        annotations:
          summary: "High network errors on {{ $labels.instance }}"
          description: |
            Network interface {{ $labels.device }} on {{ $labels.instance }} is experiencing high error rates.
            
            Error rate: {{ $value }} errors/second
            
            Troubleshooting steps:
            1. Check network cable connections
            2. Verify network switch health
            3. Review network driver logs
            4. Consider hardware replacement

  # Kubernetes é›†ç¾¤å‘Šè­¦
  - name: kubernetes.rules
    interval: 30s
    rules:
      # API Server å‘Šè­¦
      - alert: KubernetesAPIServerDown
        expr: up{job="kubernetes-apiservers"} == 0
        for: 1m
        labels:
          severity: critical
          team: platform
          component: apiserver
        annotations:
          summary: "Kubernetes API server is down"
          description: |
            Kubernetes API server {{ $labels.instance }} is not responding.
            
            Impact: 
            - Cluster management operations will fail
            - kubectl commands will not work
            - New pods cannot be scheduled
            
            Immediate actions:
            1. Check API server pod status
            2. Verify etcd connectivity
            3. Check API server logs
            4. Restart API server if necessary
      
      # etcd å‘Šè­¦
      - alert: EtcdDown
        expr: up{job="etcd"} == 0
        for: 1m
        labels:
          severity: critical
          team: platform
          component: etcd
        annotations:
          summary: "etcd cluster member is down"
          description: |
            etcd cluster member {{ $labels.instance }} is down.
            
            Impact:
            - Potential data loss risk
            - Cluster state inconsistency
            - API server may become unavailable
            
            Critical actions:
            1. Check etcd cluster health immediately
            2. Verify remaining etcd members
            3. Restore from backup if necessary
      
      # èŠ‚ç‚¹èµ„æºä¸è¶³å‘Šè­¦
      - alert: KubernetesNodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 2m
        labels:
          severity: critical
          team: platform
          component: node
        annotations:
          summary: "Kubernetes node {{ $labels.node }} is not ready"
          description: |
            Node {{ $labels.node }} has been in NotReady state for more than 2 minutes.
            
            Possible causes:
            - kubelet issues
            - Network connectivity problems
            - Resource exhaustion
            - Container runtime problems
            
            Investigation steps:
            1. Check kubelet logs
            2. Verify node resources
            3. Check container runtime status
            4. Review network connectivity
      
      # Pod é‡å¯é¢‘ç¹å‘Šè­¦
      - alert: PodCrashLooping
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15 > 5
        for: 5m
        labels:
          severity: warning
          team: application
          component: pod
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
          description: |
            Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes.
            
            Container: {{ $labels.container }}
            Namespace: {{ $labels.namespace }}
            
            Troubleshooting steps:
            1. Check pod logs for errors
            2. Review resource limits and requests
            3. Verify configuration and secrets
            4. Check liveness and readiness probes
          logs_url: "https://grafana.company.com/explore?left=%5B%22now-1h%22,%22now%22,%22Loki%22,%7B%22expr%22:%22%7Bnamespace%3D%5C%22{{ $labels.namespace }}%5C%22,pod%3D%5C%22{{ $labels.pod }}%5C%22%7D%22%7D%5D"
      
      # PVC ç©ºé—´ä¸è¶³å‘Šè­¦
      - alert: PersistentVolumeClaimSpaceLow
        expr: |
          (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) * 100 < 15
        for: 5m
        labels:
          severity: warning
          team: platform
          component: storage
        annotations:
          summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is running low on space"
          description: |
            PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} has only {{ $value | humanizePercentage }} space remaining.
            
            Available: {{ with query "kubelet_volume_stats_available_bytes{namespace='$labels.namespace',persistentvolumeclaim='$labels.persistentvolumeclaim'}" }}{{ . | first | value | humanizeBytes }}{{ end }}
            Total: {{ with query "kubelet_volume_stats_capacity_bytes{namespace='$labels.namespace',persistentvolumeclaim='$labels.persistentvolumeclaim'}" }}{{ . | first | value | humanizeBytes }}{{ end }}
            
            Actions needed:
            1. Clean up unnecessary files
            2. Expand PVC if possible
            3. Archive old data
            4. Consider storage optimization

  # åº”ç”¨ç¨‹åºå‘Šè­¦
  - name: application.rules
    interval: 30s
    rules:
      # åº”ç”¨å“åº”æ—¶é—´å‘Šè­¦
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)) > 0.5
        for: 5m
        labels:
          severity: warning
          team: application
          component: latency
        annotations:
          summary: "High response time for {{ $labels.job }}"
          description: |
            95th percentile response time for {{ $labels.job }} is {{ $value }}s.
            
            SLA threshold: 500ms
            Current value: {{ $value | humanizeDuration }}
            
            Performance investigation:
            1. Check application logs for errors
            2. Review database query performance
            3. Monitor resource utilization
            4. Consider scaling or optimization
      
      # åº”ç”¨é”™è¯¯ç‡å‘Šè­¦
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) by (job) /
          sum(rate(http_requests_total[5m])) by (job) * 100 > 5
        for: 3m
        labels:
          severity: critical
          team: application
          component: errors
        annotations:
          summary: "High error rate for {{ $labels.job }}"
          description: |
            Error rate for {{ $labels.job }} is {{ $value | humanizePercentage }}.
            
            Error threshold: 5%
            Current rate: {{ $value | humanizePercentage }}
            
            Immediate actions:
            1. Check application logs for error details
            2. Verify dependent services health
            3. Review recent deployments
            4. Consider rollback if necessary
      
      # åº”ç”¨å®ä¾‹ä¸è¶³å‘Šè­¦
      - alert: LowApplicationReplicas
        expr: |
          kube_deployment_status_replicas_available < kube_deployment_spec_replicas * 0.7
        for: 2m
        labels:
          severity: warning
          team: application
          component: availability
        annotations:
          summary: "Low replica count for {{ $labels.namespace }}/{{ $labels.deployment }}"
          description: |
            Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has only {{ $value }} available replicas.
            
            Expected: {{ with query "kube_deployment_spec_replicas{namespace='$labels.namespace',deployment='$labels.deployment'}" }}{{ . | first | value }}{{ end }}
            Available: {{ $value }}
            
            Scaling investigation:
            1. Check pod scheduling issues
            2. Verify resource availability
            3. Review node capacity
            4. Check for failed deployments
```

### å‘Šè­¦è§„åˆ™æ¨¡æ¿

```yaml
# alert-rule-template.yaml
alert_rule_template: |
  - alert: {{ .AlertName }}
    expr: {{ .Expression }}
    for: {{ .Duration | default "1m" }}
    labels:
      severity: {{ .Severity | default "warning" }}
      team: {{ .Team }}
      component: {{ .Component }}
      environment: {{ .Environment | default "production" }}
      service: {{ .Service }}
      {{- range $key, $value := .CustomLabels }}
      {{ $key }}: {{ $value }}
      {{- end }}
    annotations:
      summary: {{ .Summary }}
      description: |
        {{ .Description }}
        
        {{- if .Impact }}
        Impact: {{ .Impact }}
        {{- end }}
        
        {{- if .Troubleshooting }}
        Troubleshooting steps:
        {{- range .Troubleshooting }}
        {{ . }}
        {{- end }}
        {{- end }}
        
        {{- if .RunbookURL }}
        runbook_url: {{ .RunbookURL }}
        {{- end }}
        
        {{- if .DashboardURL }}
        dashboard_url: {{ .DashboardURL }}
        {{- end }}
        
        {{- if .LogsURL }}
        logs_url: {{ .LogsURL }}
        {{- end }}

# ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå‘Šè­¦è§„åˆ™ç¤ºä¾‹
example_alerts:
  database_connection_high:
    AlertName: "DatabaseConnectionHigh"
    Expression: "sum(database_connections_active) by (instance) > 80"
    Duration: "2m"
    Severity: "warning"
    Team: "database"
    Component: "connection-pool"
    Service: "postgresql"
    Summary: "High database connections on {{ $labels.instance }}"
    Description: |
      Database connection pool usage is {{ $value }} connections.
      This may indicate connection leaks or high load.
    Impact: "Database performance degradation and potential connection exhaustion"
    Troubleshooting:
      - "1. Check for connection leaks in applications"
      - "2. Review long-running queries"
      - "3. Consider increasing connection pool size"
      - "4. Monitor application connection patterns"
    RunbookURL: "https://runbooks.company.com/database/high-connections"
    DashboardURL: "https://grafana.company.com/d/database-overview"
```

## Alertmanager é…ç½®

### ä¸»é…ç½®æ–‡ä»¶

```yaml
# alertmanager.yml
global:
  # SMTP é…ç½®
  smtp_smarthost: 'smtp.company.com:587'
  smtp_from: 'alerts@company.com'
  smtp_auth_username: 'alerts@company.com'
  smtp_auth_password: 'your-password'
  smtp_require_tls: true
  
  # Slack é…ç½®
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
  
  # å…¶ä»–å…¨å±€é…ç½®
  resolve_timeout: 5m
  http_config:
    tls_config:
      insecure_skip_verify: false

# è·¯ç”±é…ç½®
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default-receiver'
  
  routes:
    # ä¸¥é‡å‘Šè­¦ç«‹å³é€šçŸ¥
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      group_interval: 5s
      repeat_interval: 30m
      continue: true
    
    # åŸºç¡€è®¾æ–½å‘Šè­¦
    - match:
        team: infrastructure
      receiver: 'infrastructure-team'
      group_by: ['alertname', 'instance']
      group_interval: 5m
      repeat_interval: 2h
    
    # åº”ç”¨ç¨‹åºå‘Šè­¦
    - match:
        team: application
      receiver: 'application-team'
      group_by: ['alertname', 'namespace', 'service']
      group_interval: 2m
      repeat_interval: 1h
    
    # å¹³å°å‘Šè­¦
    - match:
        team: platform
      receiver: 'platform-team'
      group_by: ['alertname', 'component']
      group_interval: 1m
      repeat_interval: 30m
    
    # æ•°æ®åº“å‘Šè­¦
    - match:
        component: database
      receiver: 'database-team'
      group_by: ['alertname', 'instance', 'database']
      group_interval: 30s
      repeat_interval: 15m
    
    # å®‰å…¨å‘Šè­¦
    - match:
        component: security
      receiver: 'security-team'
      group_wait: 0s
      group_interval: 0s
      repeat_interval: 5m
    
    # ä¸šåŠ¡å‘Šè­¦
    - match:
        type: business
      receiver: 'business-team'
      group_by: ['alertname', 'service', 'region']
      group_interval: 5m
      repeat_interval: 4h

# æŠ‘åˆ¶è§„åˆ™
inhibit_rules:
  # èŠ‚ç‚¹å®•æœºæ—¶æŠ‘åˆ¶è¯¥èŠ‚ç‚¹ä¸Šçš„å…¶ä»–å‘Šè­¦
  - source_match:
      alertname: 'NodeDown'
    target_match_re:
      instance: '.*'
    equal: ['instance']
  
  # API Server å®•æœºæ—¶æŠ‘åˆ¶ Kubernetes ç»„ä»¶å‘Šè­¦
  - source_match:
      alertname: 'KubernetesAPIServerDown'
    target_match_re:
      component: 'kubernetes.*'
    equal: ['cluster']
  
  # ä¸¥é‡é”™è¯¯æ—¶æŠ‘åˆ¶è­¦å‘Šçº§åˆ«çš„ç›¸å…³å‘Šè­¦
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service', 'instance']

# æ¥æ”¶å™¨é…ç½®
receivers:
  # é»˜è®¤æ¥æ”¶å™¨
  - name: 'default-receiver'
    slack_configs:
    - channel: '#alerts-default'
      title: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
      text: |
        {{ range .Alerts }}
        *Alert:* {{ .Annotations.summary }}
        *Description:* {{ .Annotations.description }}
        *Severity:* {{ .Labels.severity }}
        *Team:* {{ .Labels.team }}
        {{ end }}
      send_resolved: true
  
  # ä¸¥é‡å‘Šè­¦æ¥æ”¶å™¨
  - name: 'critical-alerts'
    email_configs:
    - to: 'oncall@company.com'
      subject: '[CRITICAL] {{ .GroupLabels.alertname }} - {{ .CommonLabels.service }}'
      body: |
        Alert: {{ .GroupLabels.alertname }}
        Severity: CRITICAL
        Service: {{ .CommonLabels.service }}
        Environment: {{ .CommonLabels.environment }}
        
        {{ range .Alerts }}
        Summary: {{ .Annotations.summary }}
        Description: {{ .Annotations.description }}
        Started: {{ .StartsAt }}
        {{ if .Annotations.runbook_url }}Runbook: {{ .Annotations.runbook_url }}{{ end }}
        {{ if .Annotations.dashboard_url }}Dashboard: {{ .Annotations.dashboard_url }}{{ end }}
        {{ end }}
    
    slack_configs:
    - channel: '#alerts-critical'
      title: 'ğŸš¨ CRITICAL ALERT'
      title_link: '{{ .CommonAnnotations.dashboard_url }}'
      text: |
        {{ range .Alerts }}
        *Alert:* {{ .Annotations.summary }}
        *Service:* {{ .Labels.service }}
        *Environment:* {{ .Labels.environment }}
        *Description:* {{ .Annotations.description }}
        {{ if .Annotations.runbook_url }}*Runbook:* <{{ .Annotations.runbook_url }}|View Runbook>{{ end }}
        {{ end }}
      color: 'danger'
      send_resolved: true
    
    webhook_configs:
    - url: 'https://api.pagerduty.com/generic/2010-04-15/create_event.json'
      http_config:
        tls_config:
          insecure_skip_verify: false
      send_resolved: true
  
  # åŸºç¡€è®¾æ–½å›¢é˜Ÿ
  - name: 'infrastructure-team'
    email_configs:
    - to: 'infrastructure@company.com'
      subject: '[{{ .Status | toUpper }}] Infrastructure Alert - {{ .CommonLabels.alertname }}'
      body: |
        {{ template "email.body" . }}
    
    slack_configs:
    - channel: '#infrastructure-alerts'
      title: '{{ .Status | title }} - Infrastructure Alert'
      text: |
        {{ template "slack.body" . }}
      color: '{{ if eq .Status "firing" }}warning{{ else }}good{{ end }}'
      send_resolved: true
  
  # åº”ç”¨ç¨‹åºå›¢é˜Ÿ
  - name: 'application-team'
    slack_configs:
    - channel: '#app-alerts'
      title: '{{ .Status | title }} - Application Alert'
      text: |
        {{ template "slack.body" . }}
      color: '{{ if eq .Status "firing" }}{{ if eq .CommonLabels.severity "critical" }}danger{{ else }}warning{{ end }}{{ else }}good{{ end }}'
      send_resolved: true
  
  # å¹³å°å›¢é˜Ÿ
  - name: 'platform-team'
    email_configs:
    - to: 'platform@company.com'
      subject: '[{{ .Status | toUpper }}] Platform Alert - {{ .CommonLabels.alertname }}'
      body: |
        {{ template "email.body" . }}
    
    slack_configs:
    - channel: '#platform-alerts'
      title: '{{ .Status | title }} - Platform Alert'
      text: |
        {{ template "slack.body" . }}
      send_resolved: true
  
  # æ•°æ®åº“å›¢é˜Ÿ
  - name: 'database-team'
    email_configs:
    - to: 'dba@company.com'
      subject: '[{{ .Status | toUpper }}] Database Alert - {{ .CommonLabels.alertname }}'
      body: |
        {{ template "email.body" . }}
    
    slack_configs:
    - channel: '#database-alerts'
      title: '{{ .Status | title }} - Database Alert'
      text: |
        {{ template "slack.body" . }}
      send_resolved: true
  
  # å®‰å…¨å›¢é˜Ÿ
  - name: 'security-team'
    email_configs:
    - to: 'security@company.com'
      subject: '[SECURITY ALERT] {{ .CommonLabels.alertname }}'
      body: |
        SECURITY ALERT DETECTED
        
        {{ range .Alerts }}
        Alert: {{ .Annotations.summary }}
        Description: {{ .Annotations.description }}
        Time: {{ .StartsAt }}
        Severity: {{ .Labels.severity }}
        Source: {{ .Labels.instance }}
        {{ end }}
        
        This requires immediate attention!
    
    slack_configs:
    - channel: '#security-alerts'
      title: 'ğŸ”’ SECURITY ALERT'
      text: |
        {{ range .Alerts }}
        *Alert:* {{ .Annotations.summary }}
        *Description:* {{ .Annotations.description }}
        *Source:* {{ .Labels.instance }}
        *Time:* {{ .StartsAt }}
        {{ end }}
      color: 'danger'
      send_resolved: true
  
  # ä¸šåŠ¡å›¢é˜Ÿ
  - name: 'business-team'
    email_configs:
    - to: 'business-ops@company.com'
      subject: '[{{ .Status | toUpper }}] Business Metric Alert - {{ .CommonLabels.alertname }}'
      body: |
        {{ template "email.body" . }}
    
    slack_configs:
    - channel: '#business-alerts'
      title: 'ğŸ“Š Business Alert'
      text: |
        {{ template "slack.body" . }}
      send_resolved: true

# æ¨¡æ¿å®šä¹‰
templates:
- '/etc/alertmanager/templates/*.tmpl'
```

### é€šçŸ¥æ¨¡æ¿

```yaml
# templates/email.tmpl
{{ define "email.body" }}
Alert Details:
=============

{{ range .Alerts }}
Alert Name: {{ .Labels.alertname }}
Severity: {{ .Labels.severity }}
Service: {{ .Labels.service }}{{ if .Labels.instance }}
Instance: {{ .Labels.instance }}{{ end }}{{ if .Labels.namespace }}
Namespace: {{ .Labels.namespace }}{{ end }}

Summary: {{ .Annotations.summary }}

Description:
{{ .Annotations.description }}

Started At: {{ .StartsAt.Format "2006-01-02 15:04:05 UTC" }}{{ if .EndsAt }}
Resolved At: {{ .EndsAt.Format "2006-01-02 15:04:05 UTC" }}{{ end }}

{{ if .Annotations.runbook_url }}Runbook: {{ .Annotations.runbook_url }}{{ end }}
{{ if .Annotations.dashboard_url }}Dashboard: {{ .Annotations.dashboard_url }}{{ end }}
{{ if .Annotations.logs_url }}Logs: {{ .Annotations.logs_url }}{{ end }}

Labels:
{{ range .Labels.SortedPairs }}  {{ .Name }}: {{ .Value }}
{{ end }}

{{ end }}

{{ end }}

{{ define "slack.body" }}
{{ range .Alerts }}
*Alert:* {{ .Annotations.summary }}
*Severity:* {{ .Labels.severity }}{{ if .Labels.service }}
*Service:* {{ .Labels.service }}{{ end }}{{ if .Labels.instance }}
*Instance:* {{ .Labels.instance }}{{ end }}{{ if .Labels.namespace }}
*Namespace:* {{ .Labels.namespace }}{{ end }}

*Description:* {{ .Annotations.description }}

*Started:* {{ .StartsAt.Format "2006-01-02 15:04:05 UTC" }}{{ if .EndsAt }}
*Resolved:* {{ .EndsAt.Format "2006-01-02 15:04:05 UTC" }}{{ end }}

{{ if .Annotations.runbook_url }}*Runbook:* <{{ .Annotations.runbook_url }}|View Runbook>{{ end }}
{{ if .Annotations.dashboard_url }}*Dashboard:* <{{ .Annotations.dashboard_url }}|View Dashboard>{{ end }}
{{ if .Annotations.logs_url }}*Logs:* <{{ .Annotations.logs_url }}|View Logs>{{ end }}

{{ end }}
{{ end }}

{{ define "pagerduty.title" }}
{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
{{ end }}

{{ define "pagerduty.description" }}
{{ range .Alerts }}
Alert: {{ .Annotations.summary }}
Service: {{ .Labels.service }}
Severity: {{ .Labels.severity }}
Description: {{ .Annotations.description }}
{{ end }}
{{ end }}
```

## æ™ºèƒ½å‘Šè­¦åŠŸèƒ½

### å‘Šè­¦é™å™ª

```yaml
# noise-reduction-config.yaml
noise_reduction:
  # é¢‘ç‡é™åˆ¶
  rate_limiting:
    - name: "high_frequency_alerts"
      pattern: ".*"
      max_alerts_per_minute: 10
      window_size: "5m"
      action: "suppress"
      
    - name: "duplicate_alerts"
      pattern: "same_alert_same_instance"
      max_duplicates: 3
      window_size: "1h"
      action: "group"
  
  # ç›¸ä¼¼å‘Šè­¦åˆå¹¶
  alert_clustering:
    - name: "node_resource_alerts"
      cluster_by: ["alertname", "instance"]
      similarity_threshold: 0.8
      merge_window: "10m"
      representative_selection: "first"
      
    - name: "application_errors"
      cluster_by: ["alertname", "service", "namespace"]
      similarity_threshold: 0.9
      merge_window: "5m"
      representative_selection: "highest_severity"
  
  # æ—¶é—´æ¨¡å¼åˆ†æ
  temporal_analysis:
    - name: "business_hours_suppression"
      suppress_during:
        - days: ["saturday", "sunday"]
          times: ["00:00", "23:59"]
        - days: ["monday", "tuesday", "wednesday", "thursday", "friday"]
          times: ["18:00", "08:00"]
      alert_patterns: ["low_priority_alerts"]
      
    - name: "maintenance_window"
      suppress_during:
        - days: ["sunday"]
          times: ["02:00", "04:00"]
      alert_patterns: ["infrastructure_alerts"]
  
  # ä¾èµ–å…³ç³»åˆ†æ
  dependency_analysis:
    - name: "cascade_suppression"
      root_cause_patterns:
        - "NodeDown"
        - "APIServerDown"
        - "DatabaseDown"
      dependent_patterns:
        - "PodCrashLooping"
        - "HighLatency"
        - "ServiceUnavailable"
      suppression_window: "30m"

# æœºå™¨å­¦ä¹ é™å™ª
ml_noise_reduction:
  models:
    - name: "anomaly_detector"
      type: "isolation_forest"
      features:
        - "alert_frequency"
        - "time_of_day"
        - "day_of_week"
        - "alert_duration"
        - "resolution_time"
      training_period: "30d"
      anomaly_threshold: 0.1
      
    - name: "pattern_classifier"
      type: "clustering"
      features:
        - "alert_text_similarity"
        - "label_similarity"
        - "timing_pattern"
      cluster_count: 20
      similarity_threshold: 0.7
```

### å‘Šè­¦å…³è”åˆ†æ

```yaml
# correlation-analysis.yaml
correlation_analysis:
  # æ—¶é—´åºåˆ—å…³è”
  temporal_correlation:
    - name: "infrastructure_app_correlation"
      primary_pattern: "infrastructure_alerts"
      secondary_pattern: "application_alerts"
      time_window: "15m"
      correlation_threshold: 0.8
      action: "create_incident"
      
    - name: "database_app_correlation"
      primary_pattern: "database_alerts"
      secondary_pattern: "application_performance_alerts"
      time_window: "10m"
      correlation_threshold: 0.9
      action: "enhance_context"
  
  # ç©ºé—´å…³è”ï¼ˆæœåŠ¡/èŠ‚ç‚¹ï¼‰
  spatial_correlation:
    - name: "node_pod_correlation"
      primary_pattern: "node_alerts"
      secondary_pattern: "pod_alerts"
      correlation_key: "instance"
      action: "merge_alerts"
      
    - name: "service_dependency_correlation"
      primary_pattern: "service_alerts"
      secondary_pattern: "dependent_service_alerts"
      correlation_key: "service_dependency_map"
      action: "create_service_incident"
  
  # ä¸šåŠ¡å½±å“å…³è”
  business_impact_correlation:
    - name: "customer_facing_correlation"
      primary_pattern: "customer_facing_alerts"
      business_metrics:
        - "user_login_failures"
        - "transaction_failures"
        - "page_load_errors"
      correlation_threshold: 0.85
      action: "escalate_priority"
      
    - name: "revenue_impact_correlation"
      primary_pattern: "payment_system_alerts"
      business_metrics:
        - "transaction_volume"
        - "revenue_per_minute"
        - "conversion_rate"
      correlation_threshold: 0.9
      action: "create_business_incident"

# æ ¹å› åˆ†æ
root_cause_analysis:
  analysis_rules:
    - name: "network_root_cause"
      symptoms:
        - "HighLatency"
        - "ConnectionErrors"
        - "TimeoutErrors"
      potential_causes:
        - pattern: "NetworkHardwareFailure"
          weight: 0.8
          evidence: ["NetworkErrors", "PacketLoss"]
        - pattern: "NetworkCongestion"
          weight: 0.6
          evidence: ["HighTraffic", "BandwidthUtilization"]
        - pattern: "DNSIssues"
          weight: 0.4
          evidence: ["DNSResolutionFailure", "HighDNSLatency"]
      
    - name: "resource_exhaustion_root_cause"
      symptoms:
        - "PodEviction"
        - "OOMKilled"
        - "HighMemoryUsage"
      potential_causes:
        - pattern: "MemoryLeak"
          weight: 0.9
          evidence: ["ContinuousMemoryGrowth", "ApplicationErrors"]
        - pattern: "InsufficientResources"
          weight: 0.7
          evidence: ["ResourceRequestsExceedLimits", "NodeResourceExhaustion"]
        - pattern: "ConfigurationError"
          weight: 0.5
          evidence: ["RecentConfigChange", "IncorrectResourceLimits"]
  
  machine_learning_analysis:
    model_type: "decision_tree"
    features:
      - "alert_sequence"
      - "timing_pattern"
      - "affected_components"
      - "error_patterns"
      - "resource_utilization"
    training_data: "historical_incidents"
    confidence_threshold: 0.75
```

### è‡ªåŠ¨ä¿®å¤

```yaml
# auto-remediation.yaml
auto_remediation:
  # ç®€å•ä¿®å¤åŠ¨ä½œ
  simple_actions:
    - trigger:
        alertname: "PodCrashLooping"
        conditions:
          - "restart_count < 5"
          - "last_restart_time > 5m"
      actions:
        - name: "restart_pod"
          type: "kubernetes"
          config:
            action: "delete_pod"
            namespace: "{{ .Labels.namespace }}"
            pod: "{{ .Labels.pod }}"
        - name: "collect_logs"
          type: "logging"
          config:
            namespace: "{{ .Labels.namespace }}"
            pod: "{{ .Labels.pod }}"
            lines: 1000
      
    - trigger:
        alertname: "HighMemoryUsage"
        conditions:
          - "memory_usage > 90%"
          - "hpa_enabled = true"
      actions:
        - name: "scale_up"
          type: "kubernetes"
          config:
            action: "scale_deployment"
            namespace: "{{ .Labels.namespace }}"
            deployment: "{{ .Labels.deployment }}"
            replicas: "{{ .CurrentReplicas | add 2 }}"
        - name: "monitor_scaling"
          type: "monitoring"
          config:
            duration: "10m"
            metrics: ["memory_usage", "cpu_usage"]
  
  # å¤æ‚ä¿®å¤æµç¨‹
  complex_workflows:
    - name: "database_connection_recovery"
      trigger:
        alertname: "DatabaseConnectionHigh"
        severity: "critical"
      workflow:
        - step: "analyze_connections"
          action: "query_database"
          config:
            query: "SELECT * FROM pg_stat_activity WHERE state = 'active'"
            timeout: "30s"
        - step: "kill_long_running_queries"
          condition: "query_duration > 5m"
          action: "database_command"
          config:
            command: "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state = 'active' AND query_start < NOW() - INTERVAL '5 minutes'"
        - step: "restart_connection_pool"
          action: "kubernetes"
          config:
            action: "restart_pods"
            label_selector: "app=pgbouncer"
        - step: "verify_recovery"
          action: "monitoring"
          config:
            check_metrics: ["database_connections_active"]
            expected_value: "< 50"
            timeout: "5m"
    
    - name: "service_recovery_workflow"
      trigger:
        alertname: "ServiceUnavailable"
        severity: "critical"
      workflow:
        - step: "check_pods"
          action: "kubernetes"
          config:
            action: "get_pods"
            namespace: "{{ .Labels.namespace }}"
            label_selector: "app={{ .Labels.service }}"
        - step: "restart_unhealthy_pods"
          condition: "pod_ready = false"
          action: "kubernetes"
          config:
            action: "delete_pods"
            namespace: "{{ .Labels.namespace }}"
            label_selector: "app={{ .Labels.service }}"
            field_selector: "status.phase!=Running"
        - step: "scale_if_needed"
          condition: "available_replicas < desired_replicas"
          action: "kubernetes"
          config:
            action: "scale_deployment"
            namespace: "{{ .Labels.namespace }}"
            deployment: "{{ .Labels.service }}"
            replicas: "{{ .DesiredReplicas }}"
        - step: "health_check"
          action: "http_check"
          config:
            url: "http://{{ .Labels.service }}.{{ .Labels.namespace }}.svc.cluster.local/health"
            expected_status: 200
            timeout: "30s"
        - step: "notify_success"
          action: "notification"
          config:
            message: "Service {{ .Labels.service }} has been automatically recovered"
            channels: ["slack", "email"]

# ä¿®å¤åŠ¨ä½œåº“
remediation_actions:
  kubernetes_actions:
    - name: "restart_pod"
      description: "Delete pod to trigger restart"
      risk_level: "low"
      permissions: ["pods/delete"]
      
    - name: "scale_deployment"
      description: "Scale deployment up or down"
      risk_level: "medium"
      permissions: ["deployments/scale"]
      
    - name: "rollback_deployment"
      description: "Rollback deployment to previous version"
      risk_level: "high"
      permissions: ["deployments/update"]
  
  system_actions:
    - name: "clear_disk_space"
      description: "Clean up temporary files and logs"
      risk_level: "medium"
      script: "/scripts/cleanup-disk.sh"
      
    - name: "restart_service"
      description: "Restart system service"
      risk_level: "high"
      script: "/scripts/restart-service.sh"
  
  network_actions:
    - name: "flush_dns_cache"
      description: "Clear DNS cache"
      risk_level: "low"
      script: "/scripts/flush-dns.sh"
      
    - name: "reset_network_interface"
      description: "Reset network interface"
      risk_level: "high"
      script: "/scripts/reset-network.sh"
```

## å‘Šè­¦æµ‹è¯•å’ŒéªŒè¯

### å‘Šè­¦è§„åˆ™æµ‹è¯•

```yaml
# alert-testing.yaml
alert_testing:
  # å•å…ƒæµ‹è¯•
  unit_tests:
    - name: "test_high_cpu_alert"
      rule_group: "infrastructure.rules"
      rule_name: "HighCPUUsage"
      input_series:
        - series: 'node_cpu_seconds_total{instance="node1", mode="idle"}'
          values: '0 0 0 0 0'  # 100% CPU usage (0% idle)
        - series: 'node_cpu_seconds_total{instance="node1", mode="user"}'
          values: '100 200 300 400 500'
      alert_rule: 'HighCPUUsage'
      exp_alerts:
        - exp_labels:
            severity: warning
            instance: node1
            team: infrastructure
            component: cpu
          exp_annotations:
            summary: "High CPU usage on node1"
    
    - name: "test_pod_crash_loop_alert"
      rule_group: "kubernetes.rules"
      rule_name: "PodCrashLooping"
      input_series:
        - series: 'kube_pod_container_status_restarts_total{namespace="default", pod="test-pod", container="app"}'
          values: '0 2 4 6 8 10'  # 10 restarts in 15 minutes
      alert_rule: 'PodCrashLooping'
      exp_alerts:
        - exp_labels:
            severity: warning
            namespace: default
            pod: test-pod
            container: app
            team: application
            component: pod
  
  # é›†æˆæµ‹è¯•
  integration_tests:
    - name: "test_alert_routing"
      scenario: "Critical alert routing to multiple channels"
      trigger_alert:
        alertname: "NodeDown"
        labels:
          severity: "critical"
          instance: "node1"
          team: "infrastructure"
      expected_notifications:
        - receiver: "critical-alerts"
          channels: ["email", "slack", "pagerduty"]
          delivery_time: "< 30s"
        - receiver: "infrastructure-team"
          channels: ["email", "slack"]
          delivery_time: "< 60s"
    
    - name: "test_alert_inhibition"
      scenario: "Node down should inhibit pod alerts on same node"
      trigger_alerts:
        - alertname: "NodeDown"
          labels:
            instance: "node1"
        - alertname: "PodCrashLooping"
          labels:
            instance: "node1"
      expected_behavior:
        - alert: "NodeDown"
          status: "firing"
        - alert: "PodCrashLooping"
          status: "inhibited"

# æ€§èƒ½æµ‹è¯•
performance_tests:
  load_testing:
    - name: "high_volume_alerts"
      description: "Test system with high alert volume"
      alert_rate: "1000 alerts/minute"
      duration: "10m"
      metrics:
        - "alert_processing_latency"
        - "notification_delivery_time"
        - "system_resource_usage"
      thresholds:
        - metric: "alert_processing_latency"
          threshold: "< 5s"
        - metric: "notification_delivery_time"
          threshold: "< 30s"
    
    - name: "alert_storm_handling"
      description: "Test handling of alert storms"
      scenario: "500 alerts fired simultaneously"
      expected_behavior:
        - "Rate limiting should activate"
        - "Alert grouping should occur"
        - "System should remain responsive"
      recovery_time: "< 2m"

# æ•…éšœæ¼”ç»ƒ
chaos_testing:
  alert_system_resilience:
    - name: "alertmanager_pod_failure"
      description: "Test alerting when Alertmanager pod fails"
      failure_injection:
        - target: "alertmanager-pod"
          action: "kill_pod"
      expected_behavior:
        - "Alerts continue to be processed by other replicas"
        - "No alert loss during failover"
        - "Recovery within 1 minute"
    
    - name: "prometheus_outage"
      description: "Test behavior during Prometheus outage"
      failure_injection:
        - target: "prometheus"
          action: "stop_service"
          duration: "5m"
      expected_behavior:
        - "Existing alerts continue firing"
        - "New alerts resume after recovery"
        - "No duplicate notifications"
```

### å‘Šè­¦éªŒè¯å·¥å…·

```bash
#!/bin/bash
# alert-validation.sh

# å‘Šè­¦è§„åˆ™è¯­æ³•éªŒè¯
validate_alert_rules() {
    echo "Validating Prometheus alert rules..."
    
    for file in alerts/*.yml; do
        echo "Checking $file..."
        promtool check rules "$file"
        if [ $? -ne 0 ]; then
            echo "âŒ Validation failed for $file"
            return 1
        else
            echo "âœ… $file is valid"
        fi
    done
}

# Alertmanager é…ç½®éªŒè¯
validate_alertmanager_config() {
    echo "Validating Alertmanager configuration..."
    
    amtool config show --config.file=alertmanager.yml
    if [ $? -ne 0 ]; then
        echo "âŒ Alertmanager configuration is invalid"
        return 1
    else
        echo "âœ… Alertmanager configuration is valid"
    fi
}

# å‘Šè­¦é€šçŸ¥æµ‹è¯•
test_alert_notifications() {
    echo "Testing alert notifications..."
    
    # å‘é€æµ‹è¯•å‘Šè­¦
    amtool alert add \
        --alertmanager.url=http://alertmanager:9093 \
        alertname="TestAlert" \
        severity="warning" \
        team="test" \
        summary="This is a test alert" \
        description="Testing alert notification system"
    
    echo "Test alert sent. Check notification channels."
}

# å‘Šè­¦æŸ¥è¯¢æµ‹è¯•
test_alert_queries() {
    echo "Testing alert rule queries..."
    
    # æŸ¥è¯¢å½“å‰æ´»è·ƒå‘Šè­¦
    active_alerts=$(promtool query instant \
        --server=http://prometheus:9090 \
        'ALERTS{alertstate="firing"}')
    
    echo "Active alerts: $active_alerts"
    
    # æµ‹è¯•ç‰¹å®šå‘Šè­¦è§„åˆ™
    cpu_usage=$(promtool query instant \
        --server=http://prometheus:9090 \
        '(1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100')
    
    echo "Current CPU usage: $cpu_usage"
}

# ç«¯åˆ°ç«¯æµ‹è¯•
end_to_end_test() {
    echo "Running end-to-end alert test..."
    
    # 1. è§¦å‘æµ‹è¯•å‘Šè­¦
    kubectl run test-pod --image=stress --restart=Never -- stress --cpu 8 --timeout 300s
    
    # 2. ç­‰å¾…å‘Šè­¦è§¦å‘
    sleep 120
    
    # 3. æ£€æŸ¥å‘Šè­¦çŠ¶æ€
    alerts=$(curl -s http://alertmanager:9093/api/v1/alerts | jq '.data[] | select(.labels.alertname=="HighCPUUsage")')
    
    if [ -n "$alerts" ]; then
        echo "âœ… CPU alert triggered successfully"
    else
        echo "âŒ CPU alert was not triggered"
        return 1
    fi
    
    # 4. æ¸…ç†æµ‹è¯•èµ„æº
    kubectl delete pod test-pod
    
    echo "End-to-end test completed"
}

# ä¸»å‡½æ•°
main() {
    echo "ğŸ” Starting alert system validation..."
    
    validate_alert_rules
    validate_alertmanager_config
    test_alert_notifications
    test_alert_queries
    end_to_end_test
    
    echo "ğŸ‰ Alert system validation completed"
}

main "$@"
```

## æœ€ä½³å®è·µ

### å‘Šè­¦è®¾è®¡æœ€ä½³å®è·µ

```yaml
# alerting-best-practices.yaml
alerting_best_practices:
  rule_design:
    principles:
      - "å‘Šè­¦åº”è¯¥æ˜¯å¯æ“ä½œçš„"
      - "é¿å…å‘Šè­¦ç–²åŠ³"
      - "æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯"
      - "è®¾ç½®åˆç†çš„é˜ˆå€¼å’ŒæŒç»­æ—¶é—´"
      - "åŒ…å«ä¿®å¤æŒ‡å¯¼å’Œèµ„æºé“¾æ¥"
    
    naming_conventions:
      - "ä½¿ç”¨æè¿°æ€§çš„å‘Šè­¦åç§°"
      - "åŒ…å«å½±å“çš„ç»„ä»¶æˆ–æœåŠ¡"
      - "åæ˜ å‘Šè­¦çš„ä¸¥é‡ç¨‹åº¦"
      - "ä¿æŒåç§°ç®€æ´ä¸”æ˜“ç†è§£"
    
    severity_levels:
      critical:
        description: "éœ€è¦ç«‹å³å“åº”çš„å‘Šè­¦"
        response_time: "< 5åˆ†é’Ÿ"
        examples: ["æœåŠ¡å®Œå…¨ä¸å¯ç”¨", "æ•°æ®ä¸¢å¤±é£é™©", "å®‰å…¨æ¼æ´"]
        escalation: "ç«‹å³é€šçŸ¥å€¼ç­äººå‘˜"
        
      warning:
        description: "éœ€è¦å…³æ³¨ä½†ä¸ç´§æ€¥çš„é—®é¢˜"
        response_time: "< 30åˆ†é’Ÿ"
        examples: ["æ€§èƒ½ä¸‹é™", "èµ„æºä½¿ç”¨ç‡é«˜", "éå…³é”®ç»„ä»¶æ•…éšœ"]
        escalation: "å·¥ä½œæ—¶é—´å†…å¤„ç†"
        
      info:
        description: "ä¿¡æ¯æ€§é€šçŸ¥"
        response_time: "< 2å°æ—¶"
        examples: ["éƒ¨ç½²å®Œæˆ", "å®šæœŸç»´æŠ¤", "é…ç½®å˜æ›´"]
        escalation: "è®°å½•å’Œè·Ÿè¸ª"
  
  threshold_setting:
    guidelines:
      - "åŸºäºå†å²æ•°æ®è®¾ç½®åˆç†é˜ˆå€¼"
      - "è€ƒè™‘ä¸šåŠ¡å½±å“è®¾ç½®ä¸¥é‡ç¨‹åº¦"
      - "ä½¿ç”¨ç™¾åˆ†ä½æ•°è€Œéå¹³å‡å€¼"
      - "è®¾ç½®é€‚å½“çš„è¯„ä¼°æŒç»­æ—¶é—´"
      - "å®šæœŸå®¡æŸ¥å’Œè°ƒæ•´é˜ˆå€¼"
    
    examples:
      cpu_usage:
        warning: "80%"
        critical: "95%"
        duration: "5åˆ†é’Ÿ"
        rationale: "ç»™ç³»ç»Ÿç¼“å†²æ—¶é—´å¤„ç†çªå‘è´Ÿè½½"
        
      memory_usage:
        warning: "85%"
        critical: "95%"
        duration: "3åˆ†é’Ÿ"
        rationale: "å†…å­˜ä¸è¶³ä¼šå¿«é€Ÿå½±å“ç³»ç»Ÿç¨³å®šæ€§"
        
      disk_usage:
        warning: "85%"
        critical: "95%"
        duration: "1åˆ†é’Ÿ"
        rationale: "ç£ç›˜ç©ºé—´è€—å°½ä¼šå¯¼è‡´ç³»ç»Ÿå´©æºƒ"
        
      error_rate:
        warning: "1%"
        critical: "5%"
        duration: "2åˆ†é’Ÿ"
        rationale: "é”™è¯¯ç‡ä¸Šå‡é€šå¸¸è¡¨ç¤ºä¸¥é‡é—®é¢˜"
  
  notification_strategy:
    routing_principles:
      - "æ ¹æ®å›¢é˜ŸèŒè´£è·¯ç”±å‘Šè­¦"
      - "ä¸¥é‡å‘Šè­¦å¤šæ¸ é“é€šçŸ¥"
      - "é¿å…ä¸å¿…è¦çš„å™ªéŸ³"
      - "æä¾›æ¸…æ™°çš„å‡çº§è·¯å¾„"
    
    channel_selection:
      email:
        use_case: "è¯¦ç»†ä¿¡æ¯å’Œè®°å½•ä¿å­˜"
        advantages: ["è¯¦ç»†æè¿°", "å†å²è®°å½•", "æ˜“äºè½¬å‘"]
        disadvantages: ["å“åº”è¾ƒæ…¢", "å®¹æ˜“è¢«å¿½ç•¥"]
        
      slack:
        use_case: "å›¢é˜Ÿåä½œå’Œå¿«é€Ÿå“åº”"
        advantages: ["å®æ—¶é€šçŸ¥", "å›¢é˜Ÿå¯è§", "æ˜“äºè®¨è®º"]
        disadvantages: ["å¯èƒ½è¢«å…¶ä»–æ¶ˆæ¯æ·¹æ²¡"]
        
      sms:
        use_case: "ç´§æ€¥æƒ…å†µå’Œå€¼ç­é€šçŸ¥"
        advantages: ["ç«‹å³é€è¾¾", "é«˜æ³¨æ„åº¦"]
        disadvantages: ["ä¿¡æ¯æœ‰é™", "æˆæœ¬è¾ƒé«˜"]
        
      phone:
        use_case: "æœ€ä¸¥é‡çš„ç´§æ€¥æƒ…å†µ"
        advantages: ["å¿…é¡»å“åº”", "ç›´æ¥æ²Ÿé€š"]
        disadvantages: ["ä¾µå…¥æ€§å¼º", "æˆæœ¬æœ€é«˜"]
        
      pagerduty:
        use_case: "å€¼ç­ç®¡ç†å’Œå‡çº§"
        advantages: ["è‡ªåŠ¨å‡çº§", "å€¼ç­è½®æ¢", "ç¡®è®¤è¿½è¸ª"]
        disadvantages: ["é¢å¤–æˆæœ¬", "é…ç½®å¤æ‚"]
```

## æ€»ç»“

æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿæ˜¯ Kubernetes å¯è§‚æµ‹æ€§çš„å…³é”®ç»„ä»¶ï¼Œé€šè¿‡åˆç†çš„è§„åˆ™è®¾è®¡ã€æ™ºèƒ½åŒ–çš„å¤„ç†å’Œå¤šæ ·åŒ–çš„é€šçŸ¥æœºåˆ¶ï¼Œå¯ä»¥æ˜¾è‘—æå‡è¿ç»´æ•ˆç‡å’Œç³»ç»Ÿå¯é æ€§ã€‚å…³é”®è¦ç‚¹åŒ…æ‹¬ï¼š

1. **è§„åˆ™è®¾è®¡**ï¼šåŸºäºä¸šåŠ¡å½±å“è®¾è®¡å¯æ“ä½œçš„å‘Šè­¦è§„åˆ™
2. **æ™ºèƒ½å¤„ç†**ï¼šä½¿ç”¨é™å™ªã€å…³è”å’Œè‡ªåŠ¨ä¿®å¤æŠ€æœ¯å‡å°‘å‘Šè­¦ç–²åŠ³
3. **é€šçŸ¥ç­–ç•¥**ï¼šæ ¹æ®ä¸¥é‡ç¨‹åº¦å’Œå›¢é˜ŸèŒè´£åˆç†è·¯ç”±å‘Šè­¦
4. **æŒç»­ä¼˜åŒ–**ï¼šå®šæœŸå®¡æŸ¥å’Œè°ƒæ•´å‘Šè­¦é…ç½®
5. **æµ‹è¯•éªŒè¯**ï¼šç¡®ä¿å‘Šè­¦ç³»ç»Ÿçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§

é€šè¿‡ç³»ç»Ÿæ€§çš„å‘Šè­¦ç®¡ç†å®è·µï¼Œèƒ½å¤Ÿå®ç°ä»è¢«åŠ¨å“åº”åˆ°ä¸»åŠ¨é¢„é˜²çš„è¿ç»´æ¨¡å¼è½¬å˜ã€‚
