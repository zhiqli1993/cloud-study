# 任务：生成故障排查知识文档

## 目标
为指定的技术栈/项目创建全面的故障排查文档，详细介绍问题诊断方法、常见故障场景、排查工具使用、根因分析技术等，帮助用户快速定位和解决系统故障，提升系统稳定性和运维效率。

## 任务执行要求

### 1. 目录结构
```
{project}/knowledge/06-troubleshooting/
├── README.md                     # 故障排查总览和索引
├── diagnostic-methodology.md     # 诊断方法论
├── common-issues.md              # 常见问题和解决方案
├── tools-and-commands.md         # 排查工具和命令
├── log-analysis.md               # 日志分析技巧
├── performance-issues.md         # 性能问题排查
├── network-troubleshooting.md    # 网络故障排查
├── emergency-response.md         # 应急响应流程
└── case-studies.md               # 典型故障案例
```

### 2. README.md 必须包含的内容结构

#### 2.1 故障排查概述
- **排查原则**：系统性的故障排查原则和思路
- **排查流程**：标准化的故障排查流程和步骤
- **分类体系**：故障类型的分类和特征识别
- **工具矩阵**：不同类型故障对应的排查工具

#### 2.2 故障排查架构图
```markdown
## 故障排查体系架构

使用 Mermaid 展示完整的故障排查架构：

### 故障发现层
- 监控告警（系统监控、应用监控、业务监控）
- 用户反馈（客服反馈、用户投诉、社交媒体）
- 巡检发现（定期巡检、健康检查、预防性检查）
- 自动检测（智能告警、异常检测、模式识别）

### 故障分析层
- 现象分析（故障现象收集、影响范围评估）
- 数据收集（日志收集、指标采集、配置检查）
- 关联分析（时间关联、组件关联、依赖关联）
- 根因推断（假设验证、逻辑推理、专家经验）

### 故障定位层
- 组件定位（应用组件、基础设施、第三方服务）
- 层次定位（应用层、系统层、网络层、硬件层）
- 范围定位（单点故障、区域故障、全局故障）
- 时间定位（故障开始时间、变更时间点）

### 故障解决层
- 应急处理（止损措施、服务降级、流量切换）
- 根因修复（代码修复、配置调整、资源扩容）
- 验证恢复（功能验证、性能验证、稳定性验证）
- 总结改进（复盘分析、流程优化、预防措施）
```

#### 2.3 故障分类矩阵
创建故障分类索引表格，包含：
- 故障类型（性能/可用性/功能/安全）
- 影响级别（P0/P1/P2/P3）
- 常见原因（代码/配置/资源/依赖）
- 排查难度（简单/中等/复杂）
- 排查工具
- 典型症状
- 详细文档链接

#### 2.4 关键排查场景
- **性能故障**：响应慢、吞吐量下降等性能问题
- **可用性故障**：服务不可用、间歇性故障等
- **数据故障**：数据丢失、数据不一致等问题
- **安全故障**：安全漏洞、攻击事件等安全问题
- **容量故障**：资源不足、容量瓶颈等问题

### 3. 各排查领域文档的标准结构

#### 3.1 诊断方法论（diagnostic-methodology.md）
```markdown
# 诊断方法论

## 系统性诊断方法

### MECE分析法
- **相互独立**：确保各个分析维度不重叠
- **完全穷尽**：确保覆盖所有可能的故障原因
- **分层分析**：从高层到底层逐层分析
- **假设验证**：基于假设进行验证和排除

### 5W1H分析法
- **What（什么）**：发生了什么故障现象
- **When（何时）**：故障发生的具体时间
- **Where（何处）**：故障发生的具体位置
- **Who（谁）**：故障影响的用户或系统
- **Why（为什么）**：故障发生的根本原因
- **How（如何）**：故障是如何发生和传播的

### 二分法排查
- **范围缩小**：逐步缩小故障范围
- **组件隔离**：隔离各个系统组件进行测试
- **版本回滚**：通过版本对比定位问题
- **环境对比**：对比不同环境的差异

## 故障排查流程

### 第一阶段：故障感知
- **告警接收**：及时接收和确认系统告警
- **现象收集**：收集故障的具体表现和症状
- **影响评估**：评估故障的影响范围和严重程度
- **团队召集**：根据故障级别召集相应的处理团队

### 第二阶段：快速止损
- **应急措施**：实施快速的止损和缓解措施
- **服务降级**：必要时对非核心功能进行降级
- **流量控制**：通过限流、熔断等方式保护系统
- **用户通知**：及时向用户通知故障情况

### 第三阶段：问题定位
- **数据收集**：收集相关的日志、监控数据和配置信息
- **时间线构建**：构建故障发生的详细时间线
- **假设建立**：基于现象和数据建立故障假设
- **验证测试**：通过测试验证各种假设

### 第四阶段：根因分析
- **深度分析**：深入分析故障的根本原因
- **关联分析**：分析故障与系统变更的关联性
- **依赖分析**：分析上下游依赖的影响
- **历史对比**：与历史类似故障进行对比分析

### 第五阶段：问题解决
- **解决方案设计**：设计合适的解决方案
- **风险评估**：评估解决方案的实施风险
- **分步实施**：分步骤实施解决方案
- **效果验证**：验证问题是否得到彻底解决

### 第六阶段：复盘改进
- **故障复盘**：组织详细的故障复盘会议
- **根因分析报告**：编写详细的根因分析报告
- **改进措施**：制定预防类似故障的改进措施
- **知识沉淀**：将故障经验沉淀为知识库

## 排查工具和技术

### 日志分析技术
- **结构化日志**：使用结构化格式便于分析
- **关键字搜索**：通过关键字快速定位问题
- **时间窗口分析**：分析特定时间窗口的日志
- **异常模式识别**：识别日志中的异常模式

### 监控数据分析
- **趋势分析**：分析监控指标的变化趋势
- **对比分析**：对比正常和异常时期的数据
- **关联分析**：分析不同指标之间的关联性
- **异常检测**：使用统计方法检测异常值

### 性能分析技术
- **性能剖析**：使用性能分析工具找出瓶颈
- **火焰图分析**：通过火焰图分析CPU使用情况
- **内存分析**：分析内存使用和泄漏情况
- **I/O分析**：分析磁盘和网络I/O性能
```

#### 3.2 常见问题和解决方案（common-issues.md）
```markdown
# 常见问题和解决方案

## 应用层常见问题

### 内存相关问题
- **内存泄漏**
  - 症状：应用内存使用持续增长，最终导致OOM
  - 原因：对象没有正确释放，循环引用等
  - 排查：使用内存分析工具，检查heap dump
  - 解决：修复内存泄漏代码，优化对象生命周期管理

- **内存不足**
  - 症状：OutOfMemoryError异常，应用频繁GC
  - 原因：堆内存配置过小，内存使用量超预期
  - 排查：分析内存使用模式，检查GC日志
  - 解决：调整JVM堆内存配置，优化内存使用

### 线程相关问题
- **死锁**
  - 症状：应用挂起，部分功能无响应
  - 原因：多个线程互相等待对方释放资源
  - 排查：分析线程dump，查找死锁的线程
  - 解决：重新设计锁的获取顺序，使用超时机制

- **线程池耗尽**
  - 症状：请求处理缓慢，大量请求排队
  - 原因：线程池配置不当，任务执行时间过长
  - 排查：监控线程池状态，分析任务执行时间
  - 解决：调整线程池大小，优化任务执行效率

### 数据库相关问题
- **连接池耗尽**
  - 症状：数据库连接获取失败，应用报错
  - 原因：连接池配置过小，连接泄漏
  - 排查：监控连接池状态，检查连接使用情况
  - 解决：调整连接池配置，修复连接泄漏问题

- **慢查询**
  - 症状：数据库响应慢，查询超时
  - 原因：SQL查询效率低，缺少合适索引
  - 排查：分析慢查询日志，检查执行计划
  - 解决：优化SQL语句，添加合适的索引

## 系统层常见问题

### CPU相关问题
- **CPU使用率过高**
  - 症状：系统响应慢，load average很高
  - 原因：计算密集型任务，无限循环，频繁GC
  - 排查：使用top、htop查看进程CPU使用情况
  - 解决：优化算法，修复死循环，调整GC参数

- **CPU上下文切换过多**
  - 症状：系统性能下降，CPU us时间不高但sy时间高
  - 原因：线程过多，锁竞争激烈
  - 排查：使用vmstat查看上下文切换次数
  - 解决：减少线程数量，优化锁使用

### 磁盘相关问题
- **磁盘空间不足**
  - 症状：应用无法写入文件，数据库写入失败
  - 原因：日志文件过大，临时文件未清理
  - 排查：使用df查看磁盘使用情况，du查找大文件
  - 解决：清理无用文件，配置日志轮转

- **磁盘I/O性能问题**
  - 症状：系统响应慢，iowait很高
  - 原因：磁盘性能不足，I/O密集型操作
  - 排查：使用iostat查看磁盘I/O情况
  - 解决：升级磁盘硬件，优化I/O操作

### 网络相关问题
- **网络连接数过多**
  - 症状：应用无法建立新的网络连接
  - 原因：连接没有正确关闭，连接池配置不当
  - 排查：使用netstat查看连接状态
  - 解决：修复连接泄漏，调整系统参数

- **网络延迟高**
  - 症状：网络请求响应慢，超时频繁
  - 原因：网络拥塞，路由问题，DNS解析慢
  - 排查：使用ping、traceroute、mtr等工具
  - 解决：优化网络配置，使用CDN，DNS缓存

## 中间件常见问题

### 消息队列问题
- **消息积压**
  - 症状：队列中消息数量持续增长
  - 原因：消费速度小于生产速度，消费者异常
  - 排查：监控队列长度，检查消费者状态
  - 解决：增加消费者数量，优化消费逻辑

- **消息丢失**
  - 症状：消息处理不完整，数据不一致
  - 原因：消息确认机制配置错误，消费者异常退出
  - 排查：检查消息确认配置，分析消费日志
  - 解决：正确配置消息确认，实现幂等消费

### 缓存问题
- **缓存穿透**
  - 症状：大量请求直接访问数据库，缓存命中率低
  - 原因：恶意攻击，查询不存在的数据
  - 排查：分析缓存命中率和数据库访问模式
  - 解决：设置空值缓存，使用布隆过滤器

- **缓存雪崩**
  - 症状：缓存大面积失效，数据库压力激增
  - 原因：缓存同时过期，缓存服务宕机
  - 排查：检查缓存过期策略和服务状态
  - 解决：设置随机过期时间，实现缓存高可用
```

#### 3.3 排查工具和命令（tools-and-commands.md）
```markdown
# 排查工具和命令

## 系统级排查工具

### 进程和CPU分析
- **top/htop**：实时查看系统进程和资源使用情况
  ```bash
  # 基本使用
  top
  htop
  
  # 按CPU使用率排序
  top -o %CPU
  
  # 按内存使用率排序
  top -o %MEM
  
  # 查看特定进程的线程
  top -H -p <pid>
  ```

- **ps**：查看进程状态和信息
  ```bash
  # 查看所有进程
  ps aux
  
  # 查看进程树
  ps axjf
  
  # 查看特定进程的详细信息
  ps -ef | grep <process_name>
  
  # 查看进程的线程
  ps -eLf | grep <pid>
  ```

- **pidstat**：详细的进程性能统计
  ```bash
  # 查看CPU使用情况
  pidstat -u 1
  
  # 查看内存使用情况
  pidstat -r 1
  
  # 查看I/O使用情况
  pidstat -d 1
  
  # 查看特定进程
  pidstat -p <pid> 1
  ```

### 内存分析工具
- **free**：查看系统内存使用情况
  ```bash
  # 以MB为单位显示
  free -m
  
  # 以GB为单位显示
  free -g
  
  # 持续监控
  free -s 1
  ```

- **vmstat**：虚拟内存统计信息
  ```bash
  # 基本内存统计
  vmstat
  
  # 每秒更新一次
  vmstat 1
  
  # 显示活跃和非活跃内存
  vmstat -a 1
  ```

- **pmap**：查看进程的内存映射
  ```bash
  # 查看进程内存映射
  pmap <pid>
  
  # 显示详细信息
  pmap -d <pid>
  
  # 显示设备格式
  pmap -x <pid>
  ```

### 磁盘I/O分析
- **iostat**：I/O统计信息
  ```bash
  # 基本I/O统计
  iostat
  
  # 每秒更新一次
  iostat 1
  
  # 显示扩展信息
  iostat -x 1
  
  # 只显示设备信息
  iostat -d 1
  ```

- **iotop**：实时查看I/O使用情况
  ```bash
  # 基本使用
  iotop
  
  # 只显示有I/O活动的进程
  iotop -o
  
  # 累积模式
  iotop -a
  ```

- **df/du**：磁盘空间使用情况
  ```bash
  # 查看文件系统使用情况
  df -h
  
  # 查看inode使用情况
  df -i
  
  # 查看目录大小
  du -sh <directory>
  
  # 查找大文件
  du -h <directory> | sort -hr | head -10
  ```

### 网络分析工具
- **netstat**：网络连接状态
  ```bash
  # 查看所有连接
  netstat -an
  
  # 查看TCP连接
  netstat -tan
  
  # 查看监听端口
  netstat -tln
  
  # 查看进程和端口
  netstat -tlnp
  ```

- **ss**：socket统计信息
  ```bash
  # 查看所有连接
  ss -an
  
  # 查看TCP连接
  ss -tan
  
  # 查看监听端口
  ss -tln
  
  # 显示进程信息
  ss -tlnp
  ```

- **tcpdump**：网络包捕获
  ```bash
  # 捕获特定端口的包
  tcpdump -i eth0 port 80
  
  # 捕获特定主机的包
  tcpdump -i eth0 host 192.168.1.1
  
  # 将包保存到文件
  tcpdump -i eth0 -w capture.pcap
  
  # 读取包文件
  tcpdump -r capture.pcap
  ```

## 应用级排查工具

### Java应用排查
- **jps**：查看Java进程
  ```bash
  # 查看所有Java进程
  jps -v
  
  # 显示主类名称
  jps -l
  
  # 显示JVM参数
  jps -m
  ```

- **jstack**：生成线程堆栈跟踪
  ```bash
  # 生成线程dump
  jstack <pid>
  
  # 将输出保存到文件
  jstack <pid> > thread_dump.txt
  
  # 强制生成dump
  jstack -F <pid>
  ```

- **jmap**：内存映射工具
  ```bash
  # 生成heap dump
  jmap -dump:format=b,file=heap.hprof <pid>
  
  # 查看堆内存使用情况
  jmap -heap <pid>
  
  # 查看对象直方图
  jmap -histo <pid>
  ```

- **jstat**：JVM统计信息
  ```bash
  # 查看GC情况
  jstat -gc <pid>
  
  # 持续监控GC
  jstat -gc <pid> 1s
  
  # 查看堆内存使用情况
  jstat -gccapacity <pid>
  ```

### 数据库排查工具
- **MySQL排查**
  ```sql
  -- 查看当前连接
  SHOW PROCESSLIST;
  
  -- 查看慢查询
  SHOW VARIABLES LIKE 'slow_query_log%';
  
  -- 查看锁等待
  SELECT * FROM information_schema.INNODB_LOCKS;
  
  -- 查看表状态
  SHOW TABLE STATUS;
  ```

- **Redis排查**
  ```bash
  # 查看Redis信息
  redis-cli info
  
  # 监控Redis命令
  redis-cli monitor
  
  # 查看慢查询
  redis-cli slowlog get 10
  
  # 查看内存使用
  redis-cli info memory
  ```

## 容器化环境排查工具

### Docker排查
- **docker logs**：查看容器日志
  ```bash
  # 查看容器日志
  docker logs <container_id>
  
  # 实时查看日志
  docker logs -f <container_id>
  
  # 查看最近的日志
  docker logs --tail 100 <container_id>
  ```

- **docker exec**：在容器中执行命令
  ```bash
  # 进入容器
  docker exec -it <container_id> /bin/bash
  
  # 在容器中执行命令
  docker exec <container_id> ps aux
  
  # 查看容器内的网络
  docker exec <container_id> netstat -an
  ```

- **docker stats**：容器资源使用情况
  ```bash
  # 查看所有容器的资源使用
  docker stats
  
  # 查看特定容器的资源使用
  docker stats <container_id>
  
  # 不持续更新
  docker stats --no-stream
  ```

### Kubernetes排查
- **kubectl基本命令**
  ```bash
  # 查看Pod状态
  kubectl get pods
  
  # 查看Pod详细信息
  kubectl describe pod <pod_name>
  
  # 查看Pod日志
  kubectl logs <pod_name>
  
  # 进入Pod
  kubectl exec -it <pod_name> -- /bin/bash
  ```

- **kubectl排查命令**
  ```bash
  # 查看事件
  kubectl get events --sort-by=.metadata.creationTimestamp
  
  # 查看资源使用情况
  kubectl top pods
  kubectl top nodes
  
  # 查看配置
  kubectl get configmap <name> -o yaml
  kubectl get secret <name> -o yaml
  ```

## 性能分析工具

### CPU性能分析
- **perf**：Linux性能分析工具
  ```bash
  # CPU性能分析
  perf top
  
  # 记录性能数据
  perf record -g <command>
  
  # 分析性能数据
  perf report
  
  # 生成火焰图
  perf script | stackcollapse-perf.pl | flamegraph.pl > flamegraph.svg
  ```

### 内存分析工具
- **valgrind**：内存错误检测
  ```bash
  # 内存泄漏检测
  valgrind --tool=memcheck --leak-check=full <program>
  
  # 缓存性能分析
  valgrind --tool=cachegrind <program>
  
  # 堆分析
  valgrind --tool=massif <program>
  ```

### 网络性能分析
- **iperf3**：网络性能测试
  ```bash
  # 服务器端
  iperf3 -s
  
  # 客户端测试
  iperf3 -c <server_ip>
  
  # UDP测试
  iperf3 -c <server_ip> -u
  
  # 并行连接测试
  iperf3 -c <server_ip> -P 4
  ```
```

### 4. 文档质量要求

#### 4.1 实用性要求
- **操作性强**：提供具体可执行的排查步骤和命令
- **场景完整**：覆盖常见的故障场景和排查需求
- **工具齐全**：包含各层次的排查工具和使用方法
- **案例丰富**：提供典型的故障案例和解决过程

#### 4.2 技术准确性
- **命令正确**：所有命令和脚本经过验证可以正常执行
- **参数准确**：工具的参数配置和使用方法准确无误
- **版本兼容**：确保工具和方法与技术版本兼容
- **最佳实践**：遵循行业公认的故障排查最佳实践

#### 4.3 可维护性
- **结构清晰**：文档结构清晰，便于查找和使用
- **定期更新**：根据技术发展定期更新工具和方法
- **经验沉淀**：将新的故障经验及时补充到文档中
- **团队协作**：支持团队协作和知识共享

### 5. 执行步骤

1. **故障场景分析**：分析目标技术可能出现的故障类型和场景
2. **排查方法研究**：研究系统性的故障排查方法和流程
3. **工具调研**：调研各层次的故障排查工具和使用方法
4. **案例收集**：收集典型的故障案例和解决过程
5. **流程设计**：设计标准化的故障响应和处理流程
6. **文档编写**：按标准结构编写详细的排查文档
7. **实践验证**：通过实际故障演练验证文档的有效性
8. **持续改进**：根据实际使用情况持续改进文档内容

### 6. 输出验证

完成后的故障排查文档应能够：
- 帮助团队快速定位和解决常见故障
- 提供系统性的故障排查方法和流程
- 缩短故障处理时间和恢复时间
- 提升团队的故障处理能力和经验
- 建立有效的故障响应和处理机制
- 减少类似故障的重复发生
- 为系统稳定性和可靠性提供保障

## 适用范围

此任务模板适用于需要建立故障排查能力的各种技术和场景，包括：
- 分布式系统的故障诊断和处理
- 微服务架构的问题排查
- 数据库系统的故障处理
- 网络和基础设施的故障排查
- 容器化和云原生应用的问题诊断
- 高可用系统的故障恢复
- 性能问题的分析和解决
- 任何需要快速故障定位和恢复的技术系统
